{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de modeles de vision pour la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TD, on s'interesse a fine-tuner des modeles de fondation pour classifier des images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser le jeu de donnees [Intel Image Classification](https://www.kaggle.com/datasets/puneet6060/intel-image-classification) comme exemple dans ce TD. N'hesitez pas a modifier le code ci-dessous pour essayer d'autres jeux de donnees !\n",
    "\n",
    "On commence par definir les datasets d'entrainement et de validation et les dataloaders associés.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    url_data_dir: str = './data/chest_xray',\n",
    "    batch_size: int = 16,\n",
    "    num_workers: int = 0,\n",
    "    add_augmentation: bool = False,\n",
    "    num_images: int = -1,\n",
    ") -> tuple[DataLoader, DataLoader]:\n",
    "    transform = []\n",
    "    if add_augmentation:\n",
    "        transform.append(transforms.RandomHorizontalFlip(p=0.5))\n",
    "        transform.append(transforms.RandomVerticalFlip(p=0.5))\n",
    "        transform.append(transforms.RandomResizedCrop(224, scale=(0.7, 1.0), ratio=(0.75, 1.33)))\n",
    "\n",
    "    # Taille d'image standard attendue par le model\n",
    "    transform.append(transforms.Resize((224, 224)))\n",
    "    transform.append(transforms.ToTensor())\n",
    "\n",
    "    # Le processor attend des images entre 0 et 255 :\n",
    "    transform.append(transforms.Lambda(lambda x: x * 255))\n",
    "    transform.append(transforms.Lambda(lambda x: x.to(torch.uint8)))\n",
    "    transform = transforms.Compose(transform)\n",
    "    # Pas besoin de normaliser les données, c'est géré par le processor\n",
    "    \n",
    "    train_dataset = ImageFolder(root=url_data_dir + '/train', transform=transform)\n",
    "    test_dataset = ImageFolder(root=url_data_dir + '/test', transform=transform)\n",
    "    \n",
    "    # if num_images > 0:\n",
    "    #     train_indices = np.random.choice(len(train_dataset), num_images, replace=False)\n",
    "    #     test_indices = np.random.choice(len(test_dataset), num_images // 4, replace=False)\n",
    "    #     train_dataset = Subset(train_dataset, train_indices)\n",
    "    #     test_dataset = Subset(test_dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le parametre `num_images` permet juste de limiter le nombre d'images utilisees pour l'entrainement et le test, si besoin (pour du debugging par exemple !). Notons que le dataset d'origine contient ~14000 images pour l'entrainement et ~3000 pour le test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par definir une classification head basique : une unique couche lineaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassificationHead(nn.Module):\n",
    "    '''Classification head tres basique.'''\n",
    "    def __init__(self, in_channels: int, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(in_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model class pour le fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A present, on va definir le modele que nous allons finetuner. Le modele consiste en l'enchainement d'un backbone pre-entraine, dont nous freezons les parametres, et d'une classification head dont les parametres sont entrainables.\n",
    "\n",
    "Pour correctement freezer les parametres du backbone, on peut utiliser la fonction `requires_grad` de PyTorch (`model.eval()` ne fait pas exactement la meme chose) :\n",
    "```\n",
    "for param in self.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLinearProbing(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'microsoft/resnet-50',\n",
    "        num_classes: int = 10,\n",
    "        device: str = 'cuda',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name, use_fast=False)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "        # On ajoute la classification head (ici, une couche linéaire)\n",
    "        if model_name == 'microsoft/resnet-50':\n",
    "            self.head = LinearClassificationHead(self.backbone.config.hidden_sizes[-1], num_classes)\n",
    "        else:\n",
    "            self.head = LinearClassificationHead(self.backbone.config.hidden_size, num_classes)\n",
    "\n",
    "        # On freeze le backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # On initialise les poids de la classification head\n",
    "        nn.init.xavier_uniform_(self.head.classifier.weight)\n",
    "        nn.init.zeros_(self.head.classifier.bias)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        inputs = self.processor(images=x, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.backbone(**inputs)\n",
    "        cls_tokens = outputs[1]\n",
    "        x = self.head(cls_tokens.squeeze())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On definit ensuite une fonction `train_model` qui effectue le fine-tuning du modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    lr: float = 1E-4,\n",
    "    num_epochs: int = 25,\n",
    ") -> None:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1E-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        avg_loss_train = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}: Training'):\n",
    "            images, labels = images.to(model.device), labels.to(model.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss_train += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training loss: {avg_loss_train / len(train_loader):.4f}')\n",
    "\n",
    "        scheduler.step(avg_loss_train)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        avg_loss_val = 0\n",
    "        true_labels, pred_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs}: Validation'):\n",
    "                images, labels = images.to(model.device), labels.to(model.device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                avg_loss_val += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                pred_labels.append(preds.cpu().numpy())\n",
    "                true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        pred_labels = np.concatenate(pred_labels)\n",
    "        true_labels = np.concatenate(true_labels)\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation loss: {avg_loss_val / len(val_loader):.4f}')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : lancer l'entrainement du modele, et tester divers hyperparametres. Quelle performance arrivez-vous a obtenir ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/25: Training: 100%|██████████| 326/326 [00:57<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Training loss: 1.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Validation loss: 0.7882\n",
      "Epoch 1/25, Validation accuracy: 62.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: Training: 100%|██████████| 326/326 [00:57<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Training loss: 0.5404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Validation loss: 0.6523\n",
      "Epoch 2/25, Validation accuracy: 62.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: Training: 100%|██████████| 326/326 [00:57<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Training loss: 0.4530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Validation loss: 0.5926\n",
      "Epoch 3/25, Validation accuracy: 65.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: Training: 100%|██████████| 326/326 [00:54<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Training loss: 0.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Validation loss: 0.5591\n",
      "Epoch 4/25, Validation accuracy: 69.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: Training: 100%|██████████| 326/326 [00:53<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Training loss: 0.3650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: Validation: 100%|██████████| 39/39 [00:07<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Validation loss: 0.5418\n",
      "Epoch 5/25, Validation accuracy: 71.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: Training: 100%|██████████| 326/326 [00:52<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Training loss: 0.3346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Validation loss: 0.5128\n",
      "Epoch 6/25, Validation accuracy: 72.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: Training: 100%|██████████| 326/326 [00:51<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Training loss: 0.3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Validation loss: 0.4924\n",
      "Epoch 7/25, Validation accuracy: 76.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: Training: 100%|██████████| 326/326 [00:57<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Training loss: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Validation loss: 0.4912\n",
      "Epoch 8/25, Validation accuracy: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: Training: 100%|██████████| 326/326 [00:50<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Training loss: 0.2814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: Validation: 100%|██████████| 39/39 [00:07<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Validation loss: 0.4616\n",
      "Epoch 9/25, Validation accuracy: 79.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: Training: 100%|██████████| 326/326 [00:53<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Training loss: 0.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: Validation: 100%|██████████| 39/39 [00:07<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Validation loss: 0.4854\n",
      "Epoch 10/25, Validation accuracy: 77.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: Training: 100%|██████████| 326/326 [00:53<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Training loss: 0.2603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Validation loss: 0.4735\n",
      "Epoch 11/25, Validation accuracy: 79.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: Training: 100%|██████████| 326/326 [00:50<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Training loss: 0.2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: Validation: 100%|██████████| 39/39 [00:08<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Validation loss: 0.4651\n",
      "Epoch 12/25, Validation accuracy: 79.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: Training: 100%|██████████| 326/326 [00:52<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Training loss: 0.2485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: Validation: 100%|██████████| 39/39 [00:07<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Validation loss: 0.4804\n",
      "Epoch 13/25, Validation accuracy: 78.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: Training: 100%|██████████| 326/326 [00:50<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Training loss: 0.2397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: Validation:  13%|█▎        | 5/39 [00:02<00:11,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "model = ModelLinearProbing()\n",
    "train_loader, val_loader = get_dataloaders(num_workers=16)\n",
    "lr=1E-4\n",
    "num_epochs=25\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr=lr,\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que le code peut etre simplifie en utilisant `pytorch-lightning`. La model class peut ainsi etre reecrite de la sorte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class LightningLinearProbing(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, num_classes: int = 10, lr: float = 1E-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name, use_fast=False)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        if model_name == 'microsoft/resnet-50':\n",
    "            self.head = LinearClassificationHead(self.backbone.config.hidden_sizes[-1], num_classes)\n",
    "        else:\n",
    "            self.head = LinearClassificationHead(self.backbone.config.hidden_size, num_classes)\n",
    "\n",
    "        # Freeze the backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Initialize the classification head\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.processor(images=x, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.backbone(**inputs)\n",
    "        cls_tokens = outputs[1]\n",
    "        return self.head(cls_tokens.squeeze())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        acc = accuracy(preds, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.head.parameters(), lr=self.hparams.lr,\n",
    "            betas=(0.9, 0.999), weight_decay=1E-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.2, patience=5, verbose=True\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrainement du modele se fait alors de la sorte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise le Lightning model\n",
    "lightning_model = LightningLinearProbing(model_name='facebook/dinov2-base', num_classes=6, lr=1E-4)\n",
    "\n",
    "# Puis le trainer\n",
    "trainer = pl.Trainer(max_epochs=50, gpus=1 if torch.cuda.is_available() else 0)\n",
    "\n",
    "# Et enfin on lance l'entrainement\n",
    "trainer.fit(lightning_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : comparer aux performances en kNN classification. Le fine-tuning ameliore-t-il les performances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Complexifier la tache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : modifier la classe `LinearClassificationHead` pour ajouter des couches supplémentaires, et tester les performances en finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassificationHead(nn.Module):\n",
    "    '''Une classification head plus complexe avec 3 couches lineaires.'''\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        embed_dim: int = 512,\n",
    "        num_classes: int = 2,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_channels, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Images medicales : au-dela du RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A present, on va tester les performances d'un modele de vision sur un dataset d'images medicales.\n",
    "\n",
    "On va utiliser le dataset [Chest X-Ray Pneumonia](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia) pour illustrer le cas de la classification binaire.\n",
    "\n",
    "On commence par definir les datasets d'entrainement et de validation et les dataloaders associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_dataloaders(\n",
    "    url_data_dir='./data/chest_xray/',\n",
    "    batch_size=32,\n",
    "    num_images=-1,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche quelques exemples d'images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(train_loader.dataset[i][0].permute(1, 2, 0))\n",
    "    ax.set_title(train_loader.dataset[i][1], fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de noter que les images d'origine sont en grayscale (une seule channel), mais que `torchvision.datasets.ImageFolder` les convertit automatiquement en 3 canaux (RGB) en repliquant simplement la couche de grayscale. En pratique, cela se fait via la librairie `PIL` (Python Imaging Library) avec le code suivant : `PIL.Image.open(img_path).convert('RGB')`. Dans d'autres cas (par exemple quand le dataset n'est pas formatte comme il faut pour utiliser ImageFolder), il peut etre necessaire de le faire soi-meme. Ci-dessous un code ecrit de zero pour definir son propre dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform: transforms.Compose = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # replique la single channel en RGB\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Calculer l'accuracy de classification avec kNN et en finetuning sur un modele de vision pre-entraine sur des images naturelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des modeles pre-entraines sur des images medicales existent et sont meme disponibles sur HuggingFace. Tentons a present d'utiliser le modele `https://huggingface.co/microsoft/rad-dino`. Quelle performance arrive-t-on a obtenir  en kNN et en finetuning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
